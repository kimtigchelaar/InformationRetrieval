{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f292da49",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:260%;\"> \n",
    "<b>Document Turbo <span style = \"color:#4285F4\">F</span><span style = \"color:#DB4437\">e</span><span style = \"color:#F4B400\">t</span><span style = \"color:#0F9D58\">c</span><span style = \"color:#4285F4\">h</span><span style = \"color:#DB4437\">e</span><span style = \"color:#F4B400\">r</span><span style = \"color:#0F9D58\">z</span></b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea51156",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Arjuna Beuger <br>\n",
    "Kato Schmidt <br>\n",
    "Sijf Schermerhorn <br>\n",
    "Kim Tigchelaar <br><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425778f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    üí° <b>First things first:</b> importing all of the needed libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ac097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import ujson\n",
    "import rapidjson\n",
    "import argparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import sklearn\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f8375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdbae8",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:190%\"> <span style = \"color:#4285F4\"><b>1 Dataloader</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e0bc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "We load in the data using the provided functions, which all have <i>path</i> as only argument. This path always needs to be equivalent to the path of the corresponding datafile in our folder system.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfaf09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load labels from: data/training_labels.json\n",
      "Load labels from: data/training_labels.json\n",
      "Load labels from: data/training_labels.json\n",
      "peak memory: 226.35 MiB, increment: 3.91 MiB\n",
      "Load labels from: data/validation_labels.json\n",
      "Load labels from: data/validation_labels.json\n",
      "Load labels from: data/validation_labels.json\n",
      "peak memory: 228.25 MiB, increment: -3.02 MiB\n"
     ]
    }
   ],
   "source": [
    "def passage_loader(path):\n",
    "    print(\"Load passages from: {}\".format(path))   \n",
    "    passages = ujson.load(open(path, 'r', encoding=\"utf-8\", errors=\"ignore\"))    \n",
    "    return passages\n",
    "\n",
    "def query_loader(path):    \n",
    "    print(\"Load queries from: {}\".format(path))\n",
    "    queries = ujson.load(open(path, 'r'))    \n",
    "    return queries\n",
    "\n",
    "def label_loader(path):\n",
    "    print(\"Load labels from: {}\".format(path))\n",
    "    labels = ujson.load(open(path, 'r'))    \n",
    "    return labels\n",
    "\n",
    "def index_loader(path):\n",
    "    print(\"Load passages from: {}\".format(path))   \n",
    "    index = ujson.load(open(path, 'r', encoding=\"utf-8\", errors=\"ignore\"))    \n",
    "    return index\n",
    "\n",
    "def postings_loader(path):  \n",
    "    postings = ujson.load(open(path, 'r', encoding=\"utf-8\", errors=\"ignore\"))    \n",
    "    return postings\n",
    "\n",
    "\n",
    "# %memit passages = passage_loader(\"data/passages_small.json\")\n",
    "\n",
    "# %memit queries_training = query_loader(\"data/training_queries.json\")\n",
    "# %memit queries_validation = query_loader(\"data/validation_queries.json\")\n",
    "# %memit queries_test = query_loader(\"data/test_queries.json\")\n",
    "\n",
    "%memit labels_training = label_loader(\"data/training_labels.json\")\n",
    "%memit labels_validation = label_loader(\"data/validation_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a01727",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#DB4437\"><b>2 Pre-processing</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29f86c",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:110%\"> \n",
    "In this preprocessing pipeline we take the data collections and use various methods to reduce the number of tokens by removing as much redundant data to increase the discriminatory power of the search engine. We preprocess the data using the following metrics: tokenization, stemming, removing stopwords and removing all characters not in ASCII. <br>\n",
    "    \n",
    "The tokens are filtered out if they are not in the ASCII set of characters. ASCII, also known as the American Standard Code for Information Interchange, is a very common character encoding format for text data in computers and on the internet. The ASCII table represents 128 English characters as numbers, with each letter assigned a number from 0 to 127. When pre-processing the data, characters are first encoded to their corresponding ASCII number. If it isn‚Äôt possible to encode a character, this means the character isn‚Äôt in ASCII and is therefore ignored. Lastly, the remaining characters are decoded back to their alphabetical or numerical character, which means only relevant tokens are kept. <br>\n",
    "\n",
    "By choosing to filter based on ASCII characters in stead of UTF-8, a trade-off has been made between relevance and information gain. When testing our code using UTF-8, irrelevant (or non-processable) characters such as emojis weren‚Äôt filtered out. This has negative consequences for the accuracy of our model. Additionally, since UTF-8 is a superset of all characters in widespread use today, it contains over one million codeprints, whereas ASCII only contains 128 (see the venn diagram below for a visualization). Since limited memory is a big challenge during this project, ASCII was the most rational choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11d3b9",
   "metadata": {},
   "source": [
    "<img src=\"venn.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e73ffa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Both the passages and the queries are preprocessed, using the same techniques. This doesn‚Äôt only make the code mor efficient, but also reduces the <i>vocabulary mismatch</i> problem because both the passages and the queries are reduced to their most ‚Äúcompact‚Äù form of only relevant tokens. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82de67",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>Word processing</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049f4e9",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    \n",
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>2.1 Tokenizing</b></span>\n",
    "</div>\n",
    "When tokenizing, we use the Treebank tokenizer from the nltk module. This tokenizer performs the following steps: <br>\n",
    "    \n",
    "    \n",
    "<ul>\n",
    " <li>Standard contractions are split, splitting them into two separate tokens (since they are two separate words in the first place). For example, <i>don‚Äôt</i> will be turned into <i>do</i> and <i>n‚Äôt</i>, since this is an abbreviation of two separate words. </li>\n",
    " <li>Most punctuation characters are treated as separate tokens.</li>\n",
    " <li>Commas and single quotes are split off when they are followed by whitespace.</li>\n",
    " <li>Periods that appear at the end of the line are separated. </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>2.2 Stopwords</b></span>\n",
    "</div>\n",
    "To reduce the number of tokens, all stopwords that are in the nltk‚Äôs list of English stopwords are removed.<br><br>\n",
    "    \n",
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>2.3 Stemming</b></span>\n",
    "</div>\n",
    "Reduces the tokens to their root form. For example, eating will be turned into eat. We have used the <i>PorterStemmer()</i>. This stemmer, listed in Croft et al. (2015), is the most popular algorithmic stemmer and has been used since the 1970s.\n",
    "    \n",
    "> \"The stemmer consists of a number\n",
    "of steps, each containing a set of rules for removing suffixes. At each step, the rule\n",
    "for the longest applicable suffix is executed.\" (Croft et al., 2015)\n",
    "    \n",
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>2.4 Removing non-ASCII characters</b></span>\n",
    "</div>\n",
    "Removes all tokens that contain characters not in the ASCII standard set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d5dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    tokenizer = nltk.TreebankWordTokenizer()\n",
    "    tokens = [t for s in nltk.sent_tokenize(text) for t in tokenizer.tokenize(s)]\n",
    "    tokens = [t for t in tokens if not all([c in string.punctuation for c in t])]\n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    tokens =  [PorterStemmer().stem(i) for i in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stopping(tokens):\n",
    "    stopwords =nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [i.replace(\"/\",\"-\") for i in tokens if i not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def remove_non_ascii(tokens):\n",
    "    tokens = [i.encode(\"ascii\", \"ignore\").decode() for i in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preprocess(text: str):    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = stemming(tokens)\n",
    "    tokens = stopping(tokens)\n",
    "    \n",
    "    tokens = remove_non_ascii(tokens)\n",
    "    return tokens\n",
    "\n",
    "def process_passages(passages):\n",
    "    passages_tokenised = {}\n",
    "    for passage_id in tqdm.notebook.tqdm(passages.keys()):\n",
    "        passages_tokenised[passage_id] = preprocess(passages[passage_id])\n",
    "    return passages_tokenised\n",
    "\n",
    "def process_queries(queries):\n",
    "    queries_tokenised = {}  \n",
    "    for query_id in queries.keys():\n",
    "        queries_tokenised[query_id] = preprocess(queries[query_id])\n",
    "    return queries_tokenised  \n",
    "\n",
    "\n",
    "# %memit tokenised_queries_training = process_queries(queries_training)\n",
    "# %memit tokenised_queries_validation = process_queries(queries_validation)\n",
    "# %memit tokenised_queries_test = process_queries(queries_test)\n",
    "# %memit tokenised_passages = process_passages(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f536015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %memit tokenised_passages = passage_loader(\"data/small_tokenised_passages.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb765c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    As overall output of the pre-processing, the dictionary <b>tokenised_passages</b> is returned. This dict is based on the <i>passages_small.json</i> file. The output dict is structured as following:\n",
    "</div>\n",
    "\n",
    "```\n",
    "{\"pid_123456\" :\n",
    "    [\"file\",\n",
    "     \"browser\",\n",
    "     \"appear\",\n",
    "     ...\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3495e",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#F4B400\"><b>3 Building the index</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461478b4",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "After pre-processing, the data is ready to be used. The first step in making our search engine is to build both an <i>inverted</i> and a <i>language model</i> index. The challenge of this part is that our working memory is limited, which means that we need to implement smart and efficient ways to build our indexes. To do this, we have chosen to create separate files beforehand. The folders are structured as following:\n",
    "    \n",
    "> For every character in ASCII, there is a corresponding a __folder__ with the same name.\n",
    "    >> Every folder contains a __collection of json files__ of all of the corresponding tokens that start with the character of the folder name.\n",
    "    \n",
    "For example if we create the index information for the token \"book\", a json file is created named __book.json__ containing the corresponding properties and is placed in folder __b__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25dc1",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#F4B400\"><b>3.1 Creating the files</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45d0e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "The function <b>create_files()</b> was written to create the needed    paths, folders and files beforehand. This changes based on the chosen pre-processing techniques.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf8963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(tokenised_passages, path):\n",
    "    for i in tqdm.notebook.tqdm(tokenised_passages.values()):\n",
    "        for j in i:\n",
    "            try:\n",
    "                os.mkdir(f'{path}/{j[0]}')\n",
    "            except:\n",
    "                pass\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181c19f",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#F4B400\"><b>3.2 Creating the letters</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96764fd9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "The function <b>letters()</b> is used later on to create the <i>language model</i> index.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60bf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters(tokenised_passages):\n",
    "    letter = set()\n",
    "    for i in tqdm.notebook.tqdm(tokenised_passages.values()):\n",
    "        for j in i:\n",
    "            if len(j) >0:\n",
    "                letter.add(j[0])\n",
    "    return list(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e51055",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#F4B400\"><b>3.3 The TF-IDF index</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c1dd4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    The function <b>tf_index()</b> creates the <b>inverted index</b> based on the <i>term frequency</i> and writes the json files with the needed information for every token on disk within the structured described above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2660c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_index(letter, passages):\n",
    "    postings = defaultdict(list)\n",
    "    tf_info = dict()\n",
    "    total_doc_length = 0\n",
    "    \n",
    "    for pid, passage in tqdm.notebook.tqdm(passages.items()):\n",
    "        counted = Counter(passage)\n",
    "        total_doc_length += len(passage)\n",
    "\n",
    "        for term, tf in counted.items():\n",
    "                \n",
    "            if len(term) < 200 and len(term) > 0 and term[0] == letter:\n",
    "                postings[term].append({'pid':pid, 'tf':tf, 'length_document':len(passage)})\n",
    "\n",
    "    for i in postings.keys():\n",
    "        rapidjson.dump(postings[i], open(f\"small_index/small_tf/{letter}/{i}.json\", 'w'))\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a790b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    The function <b>tf_info()</b> creates a separate file containing the term frequency information. Writing this function beforehand improves the efficiency of our code and ensures that our compulations later on can be done as fast as possible.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f880814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_info(passages, path):\n",
    "    tf_info = dict()\n",
    "    total_doc_length = 0\n",
    "    \n",
    "    for pid, passage in tqdm.notebook.tqdm(passages.items()):\n",
    "        \n",
    "        total_doc_length += len(passage)\n",
    "\n",
    "    tf_info[\"total_documents\"] = len(passages)\n",
    "    tf_info['average_doc_length'] = total_doc_length/len(passages)\n",
    "    rapidjson.dump(tf_info, open(f\"{path}/tf_info.json\", 'w'))\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fca4b",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#F4B400\"><b>3.4 The Language Model index</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1527f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    The function <b>lm_index()</b> creates the <b>inverted index</b> based on the <i>language model</i> and writes the json files with the needed information for every token on disk within the structured described above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1629c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_index(letter, tokens_dict):\n",
    "    \n",
    "    index = defaultdict(lambda:{'postings':[], 'corpus_frequency': 0})\n",
    "    tf_counter = defaultdict()\n",
    "    corpus_length = 0\n",
    "\n",
    "     \n",
    "    # Example of how to iterate over a dataframe\n",
    "    for document, token_list in tqdm.notebook.tqdm(tokens_dict.items()):\n",
    "        \n",
    "        tf_counter = Counter(token_list)\n",
    "        corpus_length += len(token_list)\n",
    "      \n",
    "        for token, tf in tf_counter.items():\n",
    "            \n",
    "            if len(token) < 200 and len(token) > 0 and token[0] == letter:\n",
    "                \n",
    "                posting = {'pid': document, 'term_frequency': tf/len(token_list)}\n",
    "                index[token]['postings'].append(posting)\n",
    "                index[token]['corpus_frequency'] += tf\n",
    "                    \n",
    "    for k,v in index.items():\n",
    "            index[k]['corpus_frequency'] = index[k]['corpus_frequency']/corpus_length\n",
    "    \n",
    "    \n",
    "    for i in tqdm.notebook.tqdm(index.keys()):\n",
    "        rapidjson.dump(index[i], open(f\"small_index/small_lm/{letter}/{i}.json\", 'w'))\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc66994",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:190%\"> \n",
    "<span style = \"color:#0F9D58\"><b>4 Creating some meta data</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb7114",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    The function <b>tf_meta()</b> creates a dictionary with meta data that is used later in the pipeline to compute calculations and therefore improves the efficiency of our code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299f5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_meta(passages):\n",
    "    \"\"\"\n",
    "    Creates a dict with meta-data\n",
    "    \"\"\"\n",
    "    \n",
    "    tf_meta = dict()\n",
    "    corpus_length = 0\n",
    "    \n",
    "    for pid, passage in tqdm.notebook.tqdm(passages.items()):\n",
    "        \n",
    "        corpus_length += len(passage)\n",
    "\n",
    "    tf_meta[\"total_documents\"] = len(passages)\n",
    "    tf_meta['average_doc_length'] = corpus_length/len(passages)\n",
    "    rapidjson.dump(tf_meta, open(f\"tf_meta.json\", 'w'))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d18b9b",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:190%\"> <span style = \"color:#4285F4\"><b>5 Ranking Algorithms</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3cd02",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:160%\"> <span style = \"color:#4285F4\"><b>5.1 TF-IDF</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790adad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(tokens, top_k=10, data_size='large'):\n",
    "    \"\"\"\n",
    "    Computes tf-idf scores\n",
    "    \"\"\"\n",
    "    title_dict = defaultdict(float)\n",
    "    if data_size == 'large':\n",
    "        path = 'large_index/postings_tf'\n",
    "    elif data_size == 'small':\n",
    "        path = 'small_index/small_tf'\n",
    "    \n",
    "    for term in tokens:\n",
    "        try:\n",
    "            index = json.load(open(f'{path}/{term[0]}/{term}.json'))\n",
    "            tf_info = json.load(open(f'{path}/tf_info.json'))\n",
    "            \n",
    "            for document in index:\n",
    "                title_dict[document['pid']] +=  (1+np.log(document['tf']))*\\\n",
    "                (np.log(tf_info['total_documents']/document['length_document']))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    titles = [(k, v) for k,v in title_dict.items()]\n",
    "\n",
    "    return sorted(titles, key=lambda m: (-m[1],m[0]))[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352d3df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    As output of the <b>search_tf_idf()</b> function, a list is returned containing tuples with for every passage id the corresponding tf-idf score. This list of tuples is sorted based on the tf-idf score, from highest to lowest. The following is an examlpe of the output structure:\n",
    "</div>\n",
    "\n",
    "```\n",
    "[(pid_123456, tf_idf),\n",
    " (pid...)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed37117",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:160%\"> <span style = \"color:#4285F4\"><b>5.2 Query Likelihood</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77684ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_likelihood(tokens, top_k=10, data_size='large'):\n",
    "    \"\"\"\n",
    "    Document Turbo Fetching!\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    query_terms = {}\n",
    "    query_frequency = defaultdict(dict)\n",
    "    \n",
    "    if data_size == 'large':\n",
    "        path = 'large_index/postings_lm'\n",
    "    elif data_size == 'small':\n",
    "        path = 'small_index/small_lm'\n",
    "   \n",
    "    for token in tokens:\n",
    "        query_terms[token] = set()    \n",
    "        try:\n",
    "            index = json.load(open(f'{path}/{token[0]}/{token}.json'))\n",
    " \n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    for document in index['postings']:\n",
    "        query_terms[token].add(document['pid'])\n",
    "        query_frequency[token][document['pid']] =  document['term_frequency']\n",
    "\n",
    "    common_docs = set()\n",
    "    \n",
    "    for x in query_terms.values():\n",
    "        if len(common_docs) == 0:\n",
    "            common_docs = x\n",
    "        else:\n",
    "            common_docs = set.intersection(common_docs, x)\n",
    "        \n",
    "    title_dict = defaultdict(float)\n",
    "    \n",
    "    for token in tokens:\n",
    "        for document in query_frequency[token]:\n",
    "            if document in common_docs:\n",
    "                if not title_dict[document]:\n",
    "                    title_dict[document] = np.log(query_frequency[token][document])\n",
    "                else:\n",
    "                    title_dict[document] += np.log(query_frequency[token][document])\n",
    "            else:\n",
    "                title_dict[document] = 0\n",
    "            \n",
    "    for k,v in title_dict.items():     \n",
    "        titles.append((k, int(v)))\n",
    "                      \n",
    "    return sorted(titles, key=lambda m: (-m[1], m[0]))[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40acce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    As output of the <b>query_likelihood()</b> function, a list is returned containing tuples with for every passage id the corresponding query likelihood score. This list of tuples is sorted based on the query likelihood score, from lowest to highest. The following is an examlpe of the output structure:\n",
    "</div>\n",
    "\n",
    "```\n",
    "[(pid_123456, query likelihood score),\n",
    " (pid...)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187e916",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:160%\"> <span style = \"color:#4285F4\"><b>5.3 Smooth Query Likelihood</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728e541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_query_likelihood(tokens, top_k=10, alpha = 0.1, data_size='large'):\n",
    "    titles = []\n",
    "    query_terms = defaultdict(set)\n",
    "    query_frequency = defaultdict(dict)\n",
    "     \n",
    "    if data_size == 'large':\n",
    "        path = 'large_index/postings_lm'\n",
    "    elif data_size == 'small':\n",
    "        path = 'small_index/small_lm'\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            index = json.load(open(f'{path}/{token[0]}/{token}.json'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for document in index['postings']:\n",
    "        query_terms[token].add(document['pid'])\n",
    "        query_frequency[token][document['pid']] =  document['term_frequency']\n",
    "            \n",
    "    for i in range(len(tokens)):\n",
    "                           \n",
    "        if query_terms[tokens[i-1]] not in query_terms[tokens[i]]:\n",
    "            for document in query_frequency[tokens[i]]:\n",
    "                if document not in query_frequency[tokens[i-1]]:\n",
    "                    query_frequency[tokens[i-1]][document] = 0      \n",
    "      \n",
    "    common_docs = set()\n",
    "    \n",
    "    for x in query_terms.values():\n",
    "        if len(common_docs) == 0:\n",
    "            common_docs = x\n",
    "        else:\n",
    "            common_docs = set.intersection(common_docs, x)\n",
    "        \n",
    "    either_docs = set().union(*query_terms.values())\n",
    "\n",
    "    title_dict = defaultdict(float)\n",
    "    \n",
    "    for token in tokens:\n",
    "        for document in query_frequency[token]:\n",
    "            \n",
    "\n",
    "            if document in common_docs:\n",
    "                if not title_dict[document]:\n",
    "                    title_dict[document] = np.log(query_frequency[token][document])\n",
    "                \n",
    "                else:\n",
    "                    title_dict[document] += query_frequency[token][document]\n",
    "          \n",
    "            elif document in either_docs:\n",
    "\n",
    "                if not title_dict[document]:\n",
    "                    title_dict[document] = np.log( (alpha * index['corpus_frequency']) + ((1-alpha) * query_frequency[token][document])) \n",
    "                else:\n",
    "                    title_dict[document] += np.log( (alpha * index['corpus_frequency']) + ((1-alpha) * query_frequency[token][document]))         \n",
    "                    \n",
    "    for k,v in title_dict.items():     \n",
    "        titles.append((k, round(v, 4)))\n",
    "    \n",
    "    return sorted(titles, key=lambda m: (-m[1], m[0]))[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2dd4a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    As output of the <b>query_likelihood()</b> function, a list is returned containing tuples with for every passage id the corresponding smooth query likelihood score. This list of tuples is sorted based on the smooth query likelihood score, from lowest to highest. The following is an examlpe of the output structure:\n",
    "</div>\n",
    "\n",
    "```\n",
    "[(pid_123456, smooth query likelihood score),\n",
    " (pid...)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039cc89",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:160%\"> <span style = \"color:#4285F4\"><b>5.4 BM25</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43b386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(tokens, k_1=1.05 , k_3=0, b=0.85 , top_k=10, data_size='large'):\n",
    "    \"\"\"\n",
    "    Computes bm25 scores\n",
    "    \"\"\"\n",
    "    title_dict = defaultdict(float)\n",
    "    \n",
    "    title_dict = defaultdict(float)\n",
    "    if data_size == 'large':\n",
    "        path = 'large_index/postings_tf'\n",
    "    elif data_size == 'small':\n",
    "        path = 'small_index/small_tf'\n",
    "    \n",
    "    for term in tokens:\n",
    "        \n",
    "        try:\n",
    "            postings = postings_loader(f'{path}/{term[0]}/{term}.json')\n",
    "            meta_data = postings_loader(f'{path}/tf_info.json')\n",
    "            doc_freq = len(postings)\n",
    "        \n",
    "            for document in postings:\n",
    "            \n",
    "                title_dict[document['pid']] +=  (((k_1+ 1) * document['tf'])/((k_1 * ((1-b)+ (b * (document['length_document']/meta_data['average_doc_length'])))) + document['tf']) * \\\n",
    "                (np.log(meta_data['total_documents']/doc_freq))  * (((k_3 + 1)*document['tf'])/(k_3 + document['tf'])))\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    \n",
    "    titles = [(k, v) for k,v in title_dict.items()]\n",
    "\n",
    "    \n",
    "    return sorted(titles, key=lambda m: (-m[1],m[0]))[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68149d56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    As output of the <b>bm25()</b> function, a list is returned containing tuples with for every passage id the corresponding bm25 score. This list of tuples is sorted based on the bm25 score, from highest to lowest. The following is an examlpe of the output structure:\n",
    "</div>\n",
    "\n",
    "```\n",
    "[(pid_123456, bm25 score),\n",
    " (pid...)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e8814",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#DB4437\"><b>6 Evaluation </b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d38ae7",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.1 Functions</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fad39",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "In the cell below, you'll find the functions corresponding to the evaluation metrics we've used to evaluate the results of our search engine: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>The dcg score</li>\n",
    "    <li>The ndcg score</li>\n",
    "    <li>The precision score</li>\n",
    "    <li>The average precision</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e6b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(scores):\n",
    "\n",
    "    dcg = []\n",
    "    \n",
    "    for i, r in np.ndenumerate(scores):\n",
    "        score = r / np.log2(i[0] + 2)\n",
    "        dcg.append(score)\n",
    "    \n",
    "    return sum(dcg)\n",
    "\n",
    "def ndcg(scores, k):\n",
    "    if dcg(scores, k) / dcg(-np.sort(-scores), k) == np.nan:\n",
    "        return 0\n",
    "    else:\n",
    "        return dcg(scores, k) / dcg(-np.sort(-scores), k)\n",
    "    \n",
    "def precision(scores, k):\n",
    "\n",
    "    if scores[:k].sum():\n",
    "        return sum(scores[:k])/len(scores[:k])\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def average_precision(scores):\n",
    "    if scores.sum():\n",
    "        return np.array([precision(scores, i+1) for i in np.nonzero(scores)[0]]).sum()/abs(scores.sum())\n",
    "    else:\n",
    "        return 0.0    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6dd23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Subsequently, we have a cell for every evaluation metric which contain functions that evaluate every ranker that we've implemented (tf-idf, query likelihood, smoothed query likelihood and BM25, respectively) on that measure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47451db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p20_map_tfidf(qid, labels, tokenised_queries):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    pids = [i[0] for i in search_tf_idf(tokenised_queries[qid], len(labels[qid]))]\n",
    "    validation = list(labels[qid].keys())\n",
    "    for i in pids:\n",
    "        if i in validation:\n",
    "            output.append(1)\n",
    "        else: output.append(0)\n",
    "    \n",
    "    return np.array(output)\n",
    "\n",
    "def p20_map_QL(qid, labels, tokenised_queries):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    pids = [i[0] for i in query_likelihood(tokenised_queries[qid], len(labels[qid]))]\n",
    "    validation = list(labels[qid].keys())\n",
    "    for i in pids:\n",
    "        if i in validation:\n",
    "            output.append(1)\n",
    "        else: output.append(0)\n",
    "    \n",
    "    return np.array(output)\n",
    "\n",
    "def p20_map_SQL(qid, labels, tokenised_queries, alpha):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    pids = [i[0] for i in smooth_query_likelihood(tokenised_queries[qid], len(labels[qid]), alpha)]\n",
    "    validation = list(labels[qid].keys())\n",
    "    for i in pids:\n",
    "        if i in validation:\n",
    "            output.append(1)\n",
    "        else: output.append(0)\n",
    "    \n",
    "    return np.array(output)\n",
    "\n",
    "def p20_map_BM25(qid, labels, tokenised_queries, k_1, k_3, b):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    pids = [i[0] for i in bm25(tokenised_queries[qid], k_1, k_3, b ,len(labels[qid]))]\n",
    "    validation = list(labels[qid].keys())\n",
    "    for i in pids:\n",
    "        if i in validation:\n",
    "            output.append(1)\n",
    "        else: output.append(0)\n",
    "    \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4712de",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.1.1 Precision@20</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560129e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Average precision@20 with the tf_idf ranking and the validation set. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15269e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision20_tfidf(tokenised_queries, labels, k):\n",
    "    \n",
    "    precisions = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        precisions.append(precision(p20_map_tfidf(qid, labels, tokenised_queries),k))\n",
    "        \n",
    "    return np.mean(precisions)       \n",
    "\n",
    "def precision20_QL(tokenised_queries, labels, k):\n",
    "    \n",
    "    precisions = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        precisions.append(precision(p20_map_QL(qid, labels, tokenised_queries),k))\n",
    "        \n",
    "    return np.mean(precisions)  \n",
    "\n",
    "def precision20_SQL(tokenised_queries, labels, k, alpha):\n",
    "    \n",
    "    precisions = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        precisions.append(precision(p20_map_SQL(qid, labels, tokenised_queries, alpha),k))\n",
    "        \n",
    "    return np.mean(precisions)    \n",
    "\n",
    "def precision20_BM25(tokenised_queries, labels, k, k_1, k_3, b):\n",
    "    \n",
    "    precisions = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        precisions.append(precision(p20_map_BM25(qid, labels, tokenised_queries, k_1, k_3, b),k))\n",
    "        \n",
    "    return np.mean(precisions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37f319",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.1.2 MAP</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa3730",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Mean-average-precision with the tf_idf ranking and the validation set. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f78183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP_tfidf(tokenised_queries, labels):\n",
    "    \n",
    "    MAP = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        MAP.append(average_precision(p20_map_tfidf(qid, labels, tokenised_queries)))\n",
    "        \n",
    "    return np.mean(MAP)\n",
    "\n",
    "def MAP_QL(tokenised_queries, labels):\n",
    "    \n",
    "    MAP = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        MAP.append(average_precision(p20_map_QL(qid, labels, tokenised_queries)))\n",
    "        \n",
    "    return np.mean(MAP)\n",
    "\n",
    "def MAP_SQL(tokenised_queries, labels, alpha):\n",
    "    \n",
    "    MAP = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        MAP.append(average_precision(p20_map_SQL(qid, labels, tokenised_queries, alpha)))\n",
    "        \n",
    "    return np.mean(MAP)\n",
    "\n",
    "def MAP_BM25(tokenised_queries, labels, k_1, k_3, b):\n",
    "    \n",
    "    MAP = []\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        MAP.append(average_precision(p20_map_BM25(qid, labels, tokenised_queries, k_1, k_3, b)))\n",
    "        \n",
    "    return np.mean(MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d0149",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.1.3 nDCG</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b41a8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Average nDCG of the top 20 predicted relevance values for all queries.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c748bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg20_tfidf(tokenised_queries, labels, k):\n",
    "    \n",
    "    real_vals = []\n",
    "    ideal_vals = []\n",
    "    ndcg_list = []\n",
    "    \n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        # welke labels staan in de top k van onze voorspelling?\n",
    "        prediction = search_tf_idf(tokenised_queries[qid], k)\n",
    "        \n",
    "        # wat zijn de echte voorspelde labels?\n",
    "        real = labels[qid]\n",
    "        \n",
    "        for p in prediction:\n",
    "            pid = p[0]\n",
    "\n",
    "            # wat is de real relevance van deze prediction voor deze query?\n",
    "            if pid in real.keys():\n",
    "                rr = real[pid]\n",
    "                real_vals.append(rr)\n",
    "                \n",
    "            # als de pid niet in de echte labels staat, dan 0 invullen \n",
    "            else:\n",
    "                real_vals.append(0)\n",
    "                \n",
    "            # de ideale optie is altijd 3, want dat is de maximale relevance\n",
    "            ideal_vals.append(3)\n",
    "            \n",
    "        ndcg = dcg(real_vals) / dcg(ideal_vals)\n",
    "        ndcg_list.append(ndcg)\n",
    "        \n",
    "        \n",
    "    return np.mean(ndcg_list)\n",
    "\n",
    "def ndcg20_QL(tokenised_queries, labels, k):\n",
    "    \n",
    "    real_vals = []\n",
    "    ideal_vals = []\n",
    "    ndcg_list = []\n",
    "    \n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        # welke labels staan in de top k van onze voorspelling?\n",
    "        prediction = query_likelihood(tokenised_queries[qid], k)\n",
    "        \n",
    "        # wat zijn de echte voorspelde labels?\n",
    "        real = labels[qid]\n",
    "        \n",
    "        for p in prediction:\n",
    "            pid = p[0]\n",
    "\n",
    "            # wat is de real relevance van deze prediction voor deze query?\n",
    "            if pid in real.keys():\n",
    "                rr = real[pid]\n",
    "                real_vals.append(rr)\n",
    "                \n",
    "            # als de pid niet in de echte labels staat, dan 0 invullen \n",
    "            else:\n",
    "                real_vals.append(0)\n",
    "                \n",
    "            # de ideale optie is altijd 3, want dat is de maximale relevance\n",
    "            ideal_vals.append(3)\n",
    "            \n",
    "        ndcg = dcg(real_vals) / dcg(ideal_vals)\n",
    "        ndcg_list.append(ndcg)\n",
    "        \n",
    "        \n",
    "    return np.mean(ndcg_list)\n",
    "\n",
    "def ndcg20_SQL(tokenised_queries, labels, k, alpha):\n",
    "    \n",
    "    real_vals = []\n",
    "    ideal_vals = []\n",
    "    ndcg_list = []\n",
    "    \n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        # welke labels staan in de top k van onze voorspelling?\n",
    "        prediction = smooth_query_likelihood(tokenised_queries[qid], k, alpha)\n",
    "        \n",
    "        # wat zijn de echte voorspelde labels?\n",
    "        real = labels[qid]\n",
    "        \n",
    "        for p in prediction:\n",
    "            pid = p[0]\n",
    "\n",
    "            # wat is de real relevance van deze prediction voor deze query?\n",
    "            if pid in real.keys():\n",
    "                rr = real[pid]\n",
    "                real_vals.append(rr)\n",
    "                \n",
    "            # als de pid niet in de echte labels staat, dan 0 invullen \n",
    "            else:\n",
    "                real_vals.append(0)\n",
    "                \n",
    "            # de ideale optie is altijd 3, want dat is de maximale relevance\n",
    "            ideal_vals.append(3)\n",
    "            \n",
    "        ndcg = dcg(real_vals) / dcg(ideal_vals)\n",
    "        ndcg_list.append(ndcg)\n",
    "        \n",
    "        \n",
    "    return np.mean(ndcg_list)\n",
    "\n",
    "def ndcg20_BM25(tokenised_queries, labels, k, k_1=1.2 , k_3=0, b=0.68):\n",
    "    \n",
    "    real_vals = []\n",
    "    ideal_vals = []\n",
    "    ndcg_list = []\n",
    "    \n",
    "    ndcg_score = []\n",
    "    \n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        # wat zijn de echte voorspelde labels?\n",
    "        real = labels[qid]\n",
    "        \n",
    "        # welke labels staan in de top k van onze voorspelling?\n",
    "        prediction = bm25(tokenised_queries[qid], k_1, k_3, b, k)\n",
    "        \n",
    "        \n",
    "        for p in prediction:\n",
    "            pid = p[0]\n",
    "\n",
    "            # wat is de real relevance van deze prediction voor deze query?\n",
    "            if pid in real.keys():\n",
    "                rr = real[pid]\n",
    "                real_vals.append(rr)\n",
    "                \n",
    "            # als de pid niet in de echte labels staat, dan 0 invullen \n",
    "            else:\n",
    "                real_vals.append(0)\n",
    "                \n",
    "            # de ideale optie is altijd 3, want dat is de maximale relevance\n",
    "            ideal_vals.append(3)\n",
    "        \n",
    "        \n",
    "\n",
    "        ndcg = dcg(real_vals) / dcg(ideal_vals)\n",
    "   \n",
    "        ndcg_list.append(ndcg)\n",
    "        \n",
    "    #print(f' k_1=1.0, b=0.85 & k_3={k_3}  --> nDCG: {np.mean(ndcg_list)}')\n",
    "        \n",
    "    return np.mean(ndcg_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22bd45",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.2 Results</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33eea69",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "After evaluating the different ranking models using the evaluation metrics above, we got some interesing results. The evaluation scores for our Query Likelihood and Smooth Query Likelihood algorithms are too low to even consider using them for our search engine. Therefore, we have focused on comparing the TF-IDF algorithm with the BM25 algorithm. This resulted in the following evaluation scores: <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3be09a",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>Evaluation of TF-IDF</b></span><br>\n",
    "<img src=\"eval_tfidf.jpeg\" alt=\"TF-IDF evaluation\" style=\"width: 500px; float: left;\"/>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5bed2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5d789",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:120%\"> \n",
    "<span style = \"color:#DB4437\"><b>Evaluation of BM25</b></span><br>\n",
    "<img src=\"eval_bm25.jpeg\" alt=\"TF-IDF evaluation\" style=\"width: 500px; float: left;\"/>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97981b19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "<b>In conclusion:</b> the BM25 algorithm has the overall best scores, which is why we've chosen to use BM25 as our ranking algorithm to use for our search engine. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4032127",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:160%\"> \n",
    "<span style = \"color:#DB4437\"><b>6.3 Questions and Answers</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f0249",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "In the milestones, several questions were asked regarding the performance of our functions. We failed to explicitly answer <i>all</i> questions, since it took us almost 3 weeks to create the index in a way that didn't crash our entire notebook and working memory. Because of this, our focus wasn't on subtle differences between provided functions and our own functions. Since we've excessively adjusted and rewritten the code since then, it isn't possible to check the performance of our first few drafts of the code. The questions that we were succesfully able to answer are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214b267",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "<dl>\n",
    "<dt>How much time do you gain by ranking documents using an inverted index, compared to the original code? Report the time it takes to run the model before and after making the index.</dt>\n",
    "<dd>According to one of the teaching assistents, before using an inverted index the model takes about 100 hours to run. After making the index, it takes about 20 minutes. Clearly, this is a huge difference.</dd><br>\n",
    "    \n",
    "<dt>Do you maintain the same value for MRR on the validation set? Report MRR on the validation set before and after making the index.</dt>\n",
    "<dd>Both before and after making the index, the MRR score is 0.245.</dd><br>\n",
    "    \n",
    "<dt>What time does it take to build the index?</dt>\n",
    "<dd>Building the index takes around 8 minutes.</dd><br>\n",
    "    \n",
    "<dt>What time does it take to retrieve the documents?</dt>\n",
    "<dd>Practically no time, short enough to be disregarded.</dd><br>\n",
    "    \n",
    "<dt>How does adding more advanced text processing techniques help your model's performance?</dt>\n",
    "<dd>When comparing different pre-processing techniques, we see that ASCII scores better than a both the most \"basic\" regex pattern we created and a regex pattern that tolerates a little bit more characters than our standard one. This is a great example of how your pre-processing technique influences your model's performance. When being too \"strict\" when filtering out characters, you lose important information and this therefore negatively impacts the overall performance of your algorithm.</dd>\n",
    "<img src=\"preprocessing.jpg\" alt=\"TF-IDF evaluation\" style=\"width: 700px; float: left;\"/><br>\n",
    "\n",
    "<br><br><br><br>\n",
    "<dt>How does stopword removal positively impact your performance?</dt>\n",
    "<dd>Removing stopwords makes sure that documents aren't falsely ranked higher because of the fact that both the query and the document contain the same stopwords (which are often used in the English language).</dd><br>\n",
    "\n",
    "<dt>Query Likelihood versus Smooth Query Likelihood</dt>\n",
    "<dd>Our (smooth) query likelihood model doesn't even remotely score as good as TF-IDF or BM25. However, if we had to choose one or the other, it would be smooth query likelihood. This is an \"expansion\" on query likelihood and adds what can be seen as an exra layer, which makes the model more complex and the result more accurate.</dd>\n",
    "\n",
    "    \n",
    "</dl>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe404c",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#F4B400\"><b>7. Creating TREC file</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e938a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "This function creates Trec files that are suitable for submitting the ranking score to Codelab. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6d2ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trec_submision(queries, k_1=1.1, k_3=0, b=0.85, top_k=100, data_size='small', type_='validation'):\n",
    "\n",
    "    \n",
    "    trec_file = \"\"\n",
    "    \n",
    "    for query_id in tqdm.notebook.tqdm(queries):\n",
    "        tokens = queries[query_id]\n",
    "        ranking = bm25(tokens, k_1, k_3, b, top_k, data_size)\n",
    "    \n",
    "            \n",
    "        for row in range(len(ranking)):\n",
    "            pid = ranking[row][0]\n",
    "            rank = row + 1\n",
    "            trec_line = f\"{query_id} {pid} {rank}\"\n",
    "            \n",
    "            trec_file += str(query_id) + \" \" + str(pid) + \" \"+ str(rank) + \"\\n \"\n",
    "\n",
    "    \n",
    "\n",
    "    text_file = open(f\"TREC/submission_{type_}.text\", 'w')\n",
    "    text_file.write(trec_file)\n",
    "    text_file.close()\n",
    "            \n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efbed9",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#0F9D58\"><b>8. Feature Construction</b></span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fefcf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "In this section the feature vectors that are used for the reranking phase will be constructed. \n",
    "    \n",
    "The feature vectors all contain the following features:\n",
    "<ul>\n",
    "    <li>tf-idf score (tfidf)</li>\n",
    "    <li>query likelihood score (QL)</li>\n",
    "    <li>bm25 score (bm25)</li>\n",
    "    <li>query term count (QTC)</li>\n",
    "    <li>document term count (DTC)</li>\n",
    "    <li>average word embedding score (AWE)</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "The following ranker computes and ranks documents based on the bm25 score but the function returns the tf-idf score and DTC as well. Since our inverted index has been constructed on disk, it takes some time to load the documents into memory. The following function has been written to expediate the process, such that the same document data has to be loaded into memory for every single feature score.\n",
    "    \n",
    "In the function <i>get_features()</i> builds a dataframe used for the feature vector. Pickle-files are stored at certain intervals to make sure the vector can be constructed incrementally.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c52f927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_plus_tfidf(tokens, k_1=1.05 , k_3=0, b=0.85 , top_k=10, data_size='large'):\n",
    "    \"\"\"\n",
    "    Computes bm25 scores\n",
    "    \"\"\"\n",
    "    \n",
    "    title_dict = defaultdict(tuple)\n",
    "    if data_size == 'large':\n",
    "        path = 'large_index/postings_tf'\n",
    "    elif data_size == 'small':\n",
    "        path = 'small_index/small_tf'\n",
    "    \n",
    "    for term in tokens:\n",
    "        \n",
    "        try:\n",
    "            postings = postings_loader(f'{path}/{term[0]}/{term}.json')\n",
    "            meta_data = postings_loader(f'{path}/tf_info.json')\n",
    "            doc_freq = len(postings)\n",
    "        \n",
    "            for document in postings:\n",
    "                \n",
    "                title_dict[document['pid']] \n",
    "                bm25 = (((k_1+ 1) * document['tf'])/((k_1 * ((1-b)+ (b * (document['length_document']/meta_data['average_doc_length'])))) + document['tf']) * \\\n",
    "                (np.log(meta_data['total_documents']/doc_freq))  * (((k_3 + 1)*document['tf'])/(k_3 + document['tf'])))\n",
    "\n",
    "                tfidf = ((1+np.log(document['tf']))*(np.log(meta_data['total_documents']/document['length_document'])))\n",
    "\n",
    "                title_dict[document['pid']] += (bm25, (tfidf, document['length_document']))\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    titles = [(k, v) for k,v in title_dict.items()]\n",
    " \n",
    "    return sorted(titles, key=lambda m: (-m[1][0],m[0]))[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6bb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker(tokenised_queries):\n",
    "    \n",
    "    short_return = dict()\n",
    "    \n",
    "    rank = defaultdict(list)\n",
    "    for qid in tqdm.notebook.tqdm(tokenised_queries.keys()):\n",
    "        \n",
    "        bm = bm25_plus(tokenised_queries[qid], top_k=100, data_size='large')\n",
    "        for x in bm:\n",
    "            \n",
    "            rank[x[1]].append((qid, x[0]),  )\n",
    "    \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdd97137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def get_features(tokenised_queries, labels, data_size='large'):\n",
    "    start_time = time.time()\n",
    "    if data_size == 'large':\n",
    "        path_tf = 'large_index/postings_tf'\n",
    "        path_lm = 'large_index/postings_lm'\n",
    "    elif data_size == 'small':\n",
    "        path_tf = 'small_index/small_tf'\n",
    "        path_lm = 'small_index/small_lm'\n",
    "        \n",
    "    \n",
    "    rows = []\n",
    "    alpha=0.1\n",
    "    \n",
    "    r = ranker(tokenised_queries)\n",
    "\n",
    "    count = 0\n",
    "    for y,x in tqdm.notebook.tqdm(r.items()):\n",
    "        qid = x[0][0]\n",
    "        pid = x[0][1]\n",
    "        count+=1\n",
    "         \n",
    "        \n",
    "        # een QL score van een qid en een pid \n",
    "        QL = 0\n",
    "        tfidf = y[1][0]\n",
    "        DTC = y[1][1]\n",
    "        \n",
    "        for token in tokenised_queries[qid]:\n",
    "            \n",
    "            try:\n",
    "                lm_index = json.load(open(f'{path_lm}/{token[0]}/{token}.json'))\n",
    "\n",
    "                for document in [x for x in lm_index['postings'] if x[\"pid\"] == pid]:\n",
    "\n",
    "                    tf = document['term_frequency']\n",
    "                    cf = lm_index['corpus_frequency']\n",
    "\n",
    "                    meta = json.load(open(f'{path_tf}/tf_info.json'))\n",
    "                    u = meta['average_doc_length']\n",
    "\n",
    "                    try:\n",
    "                        QL += np.log((tf + (u*cf)/ (DTC+u)))\n",
    "                    except:\n",
    "                        QL += np.log( (alpha * cf) + ((1-alpha) * tf))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "        if pid in labels[qid]:\n",
    "            relevance = labels[qid][pid]\n",
    "        else:\n",
    "            relevance = 0\n",
    "    \n",
    "        QTC = len(tokenised_queries[qid])\n",
    "\n",
    "\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"pid\": pid,\n",
    "            \"tfidf\": tfidf,\n",
    "            \"relevance\" : relevance,\n",
    "            \"QL\": QL,\n",
    "            \"bm25\": y[0],\n",
    "            \"QTC\": QTC,\n",
    "            \"DTC\": DTC,\n",
    "        })\n",
    "        \n",
    "        if count % 100_000 == 0:\n",
    "            temp_df = pd.DataFrame(rows)\n",
    "            temp_df.to_pickle(f'pickles/training_df.pkl')\n",
    "        \n",
    "    print(f'finished in: {time.time()-start_time} seconds')\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c311a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_test(tokenised_queries, tokenised_passages,  vectorizer, data_size='small'):\n",
    "    if data_size == 'large':\n",
    "        path_tf = 'large_index/postings_tf'\n",
    "        path_lm = 'large_index/postings_lm'\n",
    "    elif data_size == 'small':\n",
    "        path_tf = 'small_index/small_tf'\n",
    "        path_lm = 'small_index/small_lm'\n",
    "    \n",
    "    rows =[]\n",
    "    alpha=0.01\n",
    "    \n",
    "    r = ranker(tokenised_queries)\n",
    "    for y,x in tqdm.notebook.tqdm(r.items()):\n",
    "        qid = x[0][0]\n",
    "        pid = x[0][1]\n",
    "        \n",
    "        # query, title & body vectors\n",
    "        query_vector = vectorizer.transform([\" \".join(tokenised_queries[qid])])\n",
    "        passage_vector = vectorizer.transform([\"\"])\n",
    "        \n",
    "        if pid in tokenised_passages.keys():\n",
    "            passage_vector = vectorizer.transform([\" \".join(tokenised_passages[pid])])\n",
    "        \n",
    "        # DENSE query, title & body vectors\n",
    "        query_vector_dense = query_vector.todense() \n",
    "        passage_vector_dense = passage_vector.todense()\n",
    "        \n",
    "        \n",
    "        tfidf = cosine_similarity(query_vector, passage_vector)[0][0]\n",
    "         \n",
    "        # een QL score van een qid en een pid \n",
    "        QL = 0\n",
    "        tfidf_sim = 0 \n",
    "        DTC = 0\n",
    "        \n",
    "        for token in tokenised_queries[qid]:\n",
    "            try:\n",
    "                index = json.load(open(f'{path_tf}/{token[0]}/{token}.json'))\n",
    "                tf_info = json.load(open(f'{path_tf}/tf_info.json'))\n",
    "                \n",
    "                for document in index:\n",
    "                    if document['pid'] == pid: \n",
    "                        tfidf_sim +=  (1+np.log(document['tf']))*\\\n",
    "                        (np.log(tf_info['total_documents']/document['length_document']))\n",
    "            except:\n",
    "                tfidf_sim+= 0\n",
    "                \n",
    "            try:\n",
    "                index = json.load(open(f'{path_lm}/{token[0]}/{token}.json'))\n",
    "                tf = index['postings'][0]['term_frequency']\n",
    "                cf = index['corpus_frequency']\n",
    "\n",
    "                meta = json.load(open(f'{path_tf}/tf_info.json'))\n",
    "                u = meta['average_doc_length']\n",
    "\n",
    "                DTC = len(tokenised_passages[pid])\n",
    "\n",
    "                QL += np.log((tf + (u*cf)/ (DTC+u)))\n",
    "\n",
    "            except:\n",
    "                QL += np.log( (alpha * cf) + ((1-alpha) * tf))\n",
    "        \n",
    "    \n",
    "        QTC = len(tokenised_queries[qid])\n",
    "        \n",
    "        \n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"pid\": pid,\n",
    "            \"tfidf\": tfidf_sim,\n",
    "            \"tfidf similarity\": tfidf,\n",
    "            \"QL\": QL,\n",
    "            \"bm25\": y,\n",
    "            \"QTC\": QTC,\n",
    "            \"DTC\": DTC,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd8deb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3>Instead of constructing the entire feature vectors, we load in our pre-constructed feature vectors.</h3>\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b2052e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db635841",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:190%\"> <span style = \"color:#4285F4\"><b>9 AWE similarity</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b0979e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f630223",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2><u>First let's define</u></h2>\n",
    "\n",
    "**Word embeddings** <br>\n",
    "*The numerical representation of a text.*\n",
    "\n",
    "**Pretrained Word Embeddings** <br>\n",
    "Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task.\n",
    "\n",
    "Word2Vec is classified into two approaches:\n",
    "\n",
    "1. Continuous Bag-of-Words (CBOW)\n",
    "2. Skip-gram model\n",
    "\n",
    "**Continuous Bag-of-Words (CBOW) model** <br>\n",
    "Learns the focus word given the neighboring words.\n",
    "\n",
    "**Skip-gram model** <br>\n",
    "Learns the neighboring words given the focus word. \n",
    "\n",
    "*Continous Bag Of Words and Skip-gram are inverses of each other.*\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d46486",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "This function computes the average word embedding for every <b> query + doc</b> combination in <i> data</i>. We use 50 dimensions for the embeddings and only compute the AWE score if the token exists in the corpus. The corpus is created with the snippet of code below.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9849eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 20:17:14,896 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2022-10-23 20:17:14,896 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2022-10-23 20:17:14,897 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2022-10-23T20:17:14.897006', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# corpus maken waar model op getraind wordt \n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)\n",
    "            \n",
    "# sentences = MyCorpus()\n",
    "# model = Word2Vec(sentences = sentences, vector_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac47b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %memit tokenised_passages = passage_loader(\"data/large_tokenised_passages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7388e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def AWE(data, tokenised_queries, tokenised_passages, model):\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        qid = row[\"qid\"]\n",
    "        pid = row[\"pid\"]\n",
    "\n",
    "        # deze if/else is niet nodig als we large gebruiken, voor nu wel\n",
    "        if pid not in tokenised_passages.keys():   \n",
    "            sims.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # tokenised query/passage opvragen (zijn lijsten met woorden)\n",
    "            query = tokenised_queries[qid]\n",
    "            passage = tokenised_passages[pid]\n",
    "\n",
    "            # zit het woord in de corpus?\n",
    "            q = [token for token in query if token in model.wv.key_to_index]\n",
    "            p = [token for token in passage if token in model.wv.key_to_index]\n",
    "            \n",
    "            # met 1 woord kan t gemiddelde niet\n",
    "            if len(q) >= 1 and len(p) >= 1:\n",
    "                q_embedding = np.mean(model.wv[q], axis = 0)\n",
    "                p_embedding = np.mean(model.wv[p], axis = 0)\n",
    "\n",
    "                sim = cosine_similarity(q_embedding.reshape(1,-1), p_embedding.reshape(1,-1))[0][0]\n",
    "                sims.append(sim)\n",
    "            \n",
    "            else:\n",
    "                sims.append(0)\n",
    "        \n",
    "    return data.assign(AWE = sims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fca82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "As output of the <b>AWE()</b> function, a dataframe is returned with a row for every <b> query id and passage id combination</b>. The following is an examlpe of the output structure:\n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"dataframe.PNG\" alt=\"TF-IDF evaluation\" style=\"width: 600px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0c5c",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#DB4437\"><b>10 XGBRanking </b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57186130",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Firstly, the PairwiseRanker is called. Secondly, we call a function named <b>XGBranker()</b> that reranks the input dataframe based on the PairwiseRanker.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "527add9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vec trec file to df function then call xgboost and submit those results\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/best feature vecs/feature_vector_validation.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "        features.append(content[:-1])\n",
    "\n",
    "\n",
    "validation_df = pd.DataFrame(data=features, columns=['qid', 'pid', 'tfidf', 'QL', 'bm25', 'QTC','DTC', 'tfidf similarity', 'AWE', 'relevance'])\n",
    "\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/best feature vecs/feature_vector_test.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "        features.append(content[:-1])\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(data=features, columns=['qid', 'pid', 'tfidf', 'QL', 'bm25', 'QTC','DTC', 'tfidf similarity', 'AWE'])\n",
    "\n",
    "\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/best feature vecs/feature_vector_training.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "        features.append(content[:-1])\n",
    "\n",
    "\n",
    "training_df = pd.DataFrame(data=features, columns=['qid', 'pid', 'tfidf', 'QL', 'bm25', 'QTC','DTC', 'tfidf similarity', 'AWE' , 'relevance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "095ec389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRanker\n",
    "\n",
    "\n",
    "class PairwiseRanker:\n",
    "\n",
    "    def _init_(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, train_df: pd.DataFrame):\n",
    "        X_train = np.array(training_df[[\"tfidf\", \"QL\", \"bm25\", \"QTC\",\"DTC\"]].values.tolist(), dtype=object)\n",
    "        y_train = np.array(training_df.relevance.values.tolist(), dtype=object)\n",
    "        \n",
    "        group = training_df.groupby('qid').size().to_frame('size')['size'].to_numpy()\n",
    "        self.model = xgb.XGBRanker(random_state=0, objective='rank:ndcg').fit(X_train, y_train, group=group)\n",
    "        ...\n",
    "    \n",
    "    def predict(self, test_df: pd.DataFrame):\n",
    "        X_test = np.array(test_df[[\"tfidf\", \"QL\", \"bm25\", \"QTC\",\"DTC\"]].values.tolist(), dtype=object)\n",
    "        predicted_relevance = self.model.predict(X_test)\n",
    "        \n",
    "\n",
    "        \n",
    "        return predicted_relevance\n",
    "    \n",
    "reranker = PairwiseRanker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d02121bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/kz8xchm90hl_5pd2czt7spbm0000gn/T/ipykernel_31953/2129517782.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rank_df['reranker'] = ranking\n"
     ]
    }
   ],
   "source": [
    "from natsort import index_natsorted\n",
    "\n",
    "def XGBranker(df, reranker):\n",
    "    \n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    rank_df = df[['qid', 'pid']]\n",
    "    \n",
    "    reranker.fit(training_df)\n",
    "    ranking = reranker.predict(df)\n",
    "    \n",
    "    rank_df['reranker'] = ranking\n",
    "    \n",
    "    qids = sorted(set([i for i in df['qid']]))\n",
    "    \n",
    "    for i in qids:\n",
    "        small_df = rank_df[rank_df['qid'] == i]\n",
    "        df = small_df.sort_values(by=\"reranker\",\n",
    "                             key=lambda x:np.argsort(index_natsorted(-small_df[\"reranker\"])\n",
    "                                                    ))\n",
    "     \n",
    "        \n",
    "        dataframe = dataframe.append(df)\n",
    "    return dataframe\n",
    "    \n",
    "validation_results = XGBranker(validation_df, reranker)\n",
    "test_results = XGBranker(test_df, reranker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21592574",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "The function <b>trec_reranked()</b> computes the reranked DataFrame into a TREC file suitable for submitting to CodaLab.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "270ff71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfb0c4790bb458eaa7e74f9cfe2d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab05810aea240a8b22e0d57f573a72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def trec_reranked(reranked_df, file_name):\n",
    "    trec_file = \"\"\n",
    "    rank =1\n",
    "    last_qid = 'qid_8201'\n",
    "    count = 0\n",
    "    for i, row in tqdm.notebook.tqdm(reranked_df.iterrows()):\n",
    "        \n",
    "        qid = row['qid']\n",
    "        pid = row['pid']\n",
    "        \n",
    "        trec_file += f\"{qid} {pid} {rank} \\n\"\n",
    "        rank += 1\n",
    "\n",
    "    text_file = open(f\"TREC/{file_name}.text\", 'w')\n",
    "    text_file.write(trec_file)\n",
    "    text_file.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "filename = 'xgboost_ranking.zip'\n",
    "trec_reranked(validation_results, \"submission_validation\")\n",
    "trec_reranked(test_results, \"submission_test\")\n",
    "\n",
    "\n",
    "validation_file = \"TREC/submission_validation.text\"\n",
    "test_file = \"TREC/submission_test.text\"\n",
    "\n",
    "with ZipFile(\"TREC/\"+filename, 'w') as zipObj:\n",
    "    zipObj.write(validation_file, \"submission_validation.text\")\n",
    "    zipObj.write(test_file, \"submission_test.text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092b066",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#F4B400\"><b>11 Creating TREC file for RankNet</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09f91e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "The function <b>trec_ranknet()</b> computes the reranked DataFrame into a TREC file.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trec_ranknet(df_awe, file_name):\n",
    "    trec_file = \"\"\n",
    "\n",
    "    for i, row in tqdm.notebook.tqdm(df_awe.iterrows()):\n",
    "\n",
    "        qid = row['qid']\n",
    "        pid = row['pid']\n",
    "        relevance = row['relevance']\n",
    "        tf = row['tfidf']\n",
    "        ql = row['QL']\n",
    "        bm = row[\"bm25\"]\n",
    "        QTC = row['QTC']\n",
    "        DTC = row['DTC']\n",
    "        AWE = row['AWE']\n",
    "\n",
    "        trec_file += f\"{qid} {pid} {tf} {ql} {bm} {QTC} {DTC} {AWE} \\n\"\n",
    "\n",
    "\n",
    "    text_file = open(f\"output/{file_name}.text\", 'w')\n",
    "    text_file.write(trec_file)\n",
    "    text_file.close()\n",
    "    \n",
    "    return \n",
    "\n",
    "# trec_ranknet(data_awe_validation, 'feature_vector_validation')\n",
    "\n",
    "def trec_ranknet_test(df_awe, file_name):\n",
    "    \n",
    "    trec_file = \"\"\n",
    "\n",
    "    for i, row in tqdm.notebook.tqdm(df_awe.iterrows()):\n",
    "\n",
    "        qid = row['qid']\n",
    "        pid = row['pid']\n",
    "        tf = row['tfidf']\n",
    "        ql = row['QL']\n",
    "        bm = row[\"bm25\"]\n",
    "        QTC = row['QTC']\n",
    "        DTC = row['DTC']\n",
    "        AWE = row['AWE']\n",
    "\n",
    "        trec_file += f\"{qid} {pid} {tf} {ql} {bm} {QTC} {DTC} {AWE} \\n\"\n",
    "\n",
    "    \n",
    "    text_file = open(f\"output/{file_name}.text\", 'w')\n",
    "    text_file.write(trec_file)\n",
    "    text_file.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9084ebb",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#0F9D58\"><b>12 RankNet</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3346d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "The code below calls the RankNet module and reranks our data using this neural network. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for RankNet\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", type=int, default=15)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--input_size\", type=int, default=5)\n",
    "parser.add_argument(\"--hidden_size1\", type=int, default=128)\n",
    "parser.add_argument(\"--hidden_size2\", type=int, default=128)\n",
    "parser.add_argument(\"--output_size\", type=int, default=1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "parser.add_argument(\"--random_seed\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716e71c",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "    Also, we need to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.cuda.manual_seed_all(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranknet import train, inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260137a",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Next, you need to train RankNet on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3243fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the training set.\n",
    "\"{qid} {pid} {tf} {ql} {bm} {QTC} {DTC}  {AWE} \\n\"\n",
    "\"            [2]  [3]  [4]   [5]   [6]    [8]       \"\n",
    "\n",
    "q_id = []\n",
    "features = []\n",
    "labels = []\n",
    "        \n",
    "print(\"Load file {}\".format(\"output/best feature vecs/feature_vector_training.text\"))\n",
    "with codecs.open(\"output/best feature vecs/feature_vector_training.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "       \n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[2]), float(content[3]), float(content[4]), float(content[5]), float(content[6])])\n",
    "        labels.append(labels_training[content[0]][content[1]] if content[1] in labels_training[content[0]] else 0)\n",
    "\n",
    "# train model\n",
    "%memit train(args, q_id, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a875393",
   "metadata": {},
   "source": [
    "Then, you need to conduct inference on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the validation set.\n",
    "\n",
    "print(\"Load file {}\".format(\"output/feature_vector_validation.text\")) \n",
    "q_id = []\n",
    "p_id = []\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/feature_vector_validation.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[2]), float(content[3]), float(content[4]), float(content[5]), float(content[6])])\n",
    "        p_id.append(content[1])\n",
    "\n",
    "# conduct inference on the validation set.\n",
    "%memit scores = inference(args, q_id, p_id, features) \n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "        \n",
    "with codecs.open(\"output/re_ranking_validation_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score:\n",
    "            ranking+=1           \n",
    "                    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(score), \"re_ranking_on_the_validation_set\"])+os.linesep)\n",
    "\n",
    "# output the result file. \n",
    "print(\"Produce file {}\".format(\"re_ranking_validation_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a197e6f",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "Similarly, you need to conduct inference on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the test set.\n",
    "\n",
    "print(\"Load file {}\".format(\"output/feature_vector_test_result.text\")) \n",
    "q_id = []\n",
    "p_id = []\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/feature_vector_test.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split(' ')\n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[2]), float(content[3]), float(content[4]), float(content[5]), float(content[6])])\n",
    "        p_id.append(content[1])\n",
    "\n",
    "# conduct inference on the validation set.\n",
    "%memit scores = inference(args, q_id, p_id, features) \n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "        \n",
    "with codecs.open(\"output/re_ranking_test_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score:\n",
    "            ranking+=1           \n",
    "                    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(score), \"re_ranking_on_the_test_set\"])+os.linesep)\n",
    "\n",
    "# output the result file. \n",
    "print(\"Produce file {}\".format(\"re_ranking_test_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1432f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\", style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "The overall output of the code above are two TREC files reranked by RankNet.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec6286",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2><u>The following cells will create the re-ranked lists for the validation and test sets in TREC format.</u></h2>\n",
    "\n",
    "They have a qid-pid-rank structure\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c120556",
   "metadata": {},
   "source": [
    "<div id = \"data\", style = \"font-family: Helvetica Neue;; font-size:190%\"> <span style = \"color:#4285F4\"><b>13 Conclusion</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7dc56",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue; font-size:110%\"> \n",
    "The submission files have been created. They can be submitted to for testing and scoring. In our  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728c360",
   "metadata": {},
   "source": [
    "<div style = \"font-family: Helvetica Neue;; font-size:190%\"> \n",
    "<span style = \"color:#DB4437\"><b>14 Submission to CodaLab Leaderboard</b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f92ea",
   "metadata": {},
   "source": [
    "query_id, passage_id , rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf69257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "studentnumber = \"result\"\n",
    "studentname = \"DTF\"\n",
    "\n",
    "# Filename of submission Zip Archive to upload to CodaLab\n",
    "filename = f\"{studentnumber}_{studentname}_codalab_submission.zip\"\n",
    "\n",
    "# Filename of submission .text file from validation and test sets\n",
    "# please make sure these are the rankings you want to submit, and make sure they are in the proper format\n",
    "# Format: query_id    passage_id    rank     ... other things you write in a line are not taken into account\n",
    "validation_file = \"output/re_ranking_validation_result.text\"\n",
    "test_file = \"output/re_ranking_test_result.text\"\n",
    "\n",
    "with ZipFile(\"output/\"+filename, 'w') as zipObj:\n",
    "    zipObj.write(validation_file, \"submission_validation.text\")\n",
    "    zipObj.write(test_file, \"submission_test.text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
